--- spellcaster_portal.py
+++ spellcaster_portal.py (patched)
@@ -380,6 +380,64 @@
     }
 
 # ---- Helpers --------------------------------------------------------------
+
+def index_single_pdf(pdf_path: Path) -> dict:
+    """Index or update a single PDF in the SQLite library DB.
+
+    Returns a small status dict: {status: 'indexed'|'skipped'|'error', relpath: str, reason?: str}
+    """
+    try:
+        if not pdf_path.exists() or not pdf_path.is_file():
+            return {"status": "error", "relpath": str(pdf_path), "reason": "file_not_found"}
+        if pdf_path.suffix.lower() != ".pdf":
+            return {"status": "skipped", "relpath": str(pdf_path), "reason": "not_pdf"}
+
+        relpath = str(pdf_path.relative_to(LIB_DIR)).replace("\\", "/")
+
+        data = pdf_path.read_bytes()
+        file_hash = hashlib.md5(data).hexdigest()
+
+        title = pdf_path.stem
+        kind = "pdf"
+        content = ""  # we don't extract text here (fast + safe). Use /api/reindex for full rebuild if you add extractors.
+
+        now = datetime.utcnow().isoformat(timespec="seconds") + "Z"
+
+        conn = get_conn()
+        cur = conn.cursor()
+
+        cur.execute("SELECT id, hash FROM documents WHERE relpath = ? AND kind = 'pdf'", (relpath,))
+        row = cur.fetchone()
+
+        if row and row[1] == file_hash:
+            return {"status": "skipped", "relpath": relpath, "reason": "unchanged"}
+
+        if row:
+            doc_id = row[0]
+            cur.execute(
+                "UPDATE documents SET title = ?, content = ?, hash = ?, updated_at = ? WHERE id = ?",
+                (title, content, file_hash, now, doc_id),
+            )
+        else:
+            cur.execute(
+                "INSERT INTO documents (title, relpath, kind, content, hash, added_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
+                (title, relpath, kind, content, file_hash, now, now),
+            )
+
+        conn.commit()
+        return {"status": "indexed", "relpath": relpath}
+    except Exception as e:
+        try:
+            conn.rollback()
+        except Exception:
+            pass
+        return {"status": "error", "relpath": str(pdf_path), "reason": str(e)}
+    finally:
+        try:
+            conn.close()
+        except Exception:
+            pass
+
 
 def rel_to_abs(rel_path: str) -> Path:
     p = Path(rel_path)
@@ -985,6 +1043,158 @@
     return jsonify({"status": "ok", "use_cleaner": use_cleaner, **summary})
 
 # ---- Assets ---------------------------------------------------------------
+
+# -----------------------------
+# Resumable (chunked) uploads
+# -----------------------------
+
+UPLOADS_DIR = BASE_DIR / "uploads"
+UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
+
+def _upload_state_path(upload_id: str) -> Path:
+    return UPLOADS_DIR / f"{upload_id}.json"
+
+def _upload_part_path(upload_id: str) -> Path:
+    return UPLOADS_DIR / f"{upload_id}.part"
+
+def _safe_relpath(relpath: str) -> str:
+    # Prevent path traversal; keep relative to library folder.
+    relpath = (relpath or "").replace("\\", "/").lstrip("/")
+    relpath = "/".join([p for p in relpath.split("/") if p not in ("", ".", "..")])
+    return relpath
+
+@app.post("/api/upload/init")
+def api_upload_init():
+    """Start a resumable upload session.
+
+    JSON body:
+      - filename: original filename (required)
+      - size: total size in bytes (required)
+      - relpath: relative path under library/ (optional; defaults to filename)
+    """
+    data = request.get_json(silent=True) or {}
+    filename = (data.get("filename") or "").strip()
+    total_size = int(data.get("size") or 0)
+    relpath = _safe_relpath(data.get("relpath") or filename)
+
+    if not filename or total_size <= 0:
+        return jsonify({"ok": False, "error": "filename_and_size_required"}), 400
+
+    upload_id = uuid.uuid4().hex
+    chunk_size = int(os.getenv("UPLOAD_CHUNK_SIZE", str(5 * 1024 * 1024)))  # 5MB default (Render-friendly)
+
+    state = {
+        "upload_id": upload_id,
+        "filename": filename,
+        "relpath": relpath,
+        "total_size": total_size,
+        "chunk_size": chunk_size,
+        "received": [],  # list[int] chunk indexes
+        "created_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
+        "updated_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
+    }
+    _upload_state_path(upload_id).write_text(json.dumps(state), encoding="utf-8")
+    # Create empty part file
+    with open(_upload_part_path(upload_id), "ab"):
+        pass
+
+    return jsonify({"ok": True, "upload_id": upload_id, "chunk_size": chunk_size, "received": []})
+
+@app.get("/api/upload/status/<upload_id>")
+def api_upload_status(upload_id: str):
+    sp = _upload_state_path(upload_id)
+    if not sp.exists():
+        return jsonify({"ok": False, "error": "not_found"}), 404
+    state = json.loads(sp.read_text(encoding="utf-8"))
+    part = _upload_part_path(upload_id)
+    state["part_size"] = part.stat().st_size if part.exists() else 0
+    return jsonify({"ok": True, **state})
+
+@app.post("/api/upload/chunk/<upload_id>")
+def api_upload_chunk(upload_id: str):
+    """Upload one chunk.
+
+    Form-data:
+      - chunk: file (required)
+      - index: int (required)
+    """
+    sp = _upload_state_path(upload_id)
+    if not sp.exists():
+        return jsonify({"ok": False, "error": "not_found"}), 404
+
+    state = json.loads(sp.read_text(encoding="utf-8"))
+    chunk_file = request.files.get("chunk")
+    if chunk_file is None:
+        return jsonify({"ok": False, "error": "chunk_required"}), 400
+
+    try:
+        idx = int(request.form.get("index", "-1"))
+    except Exception:
+        idx = -1
+
+    if idx < 0:
+        return jsonify({"ok": False, "error": "index_required"}), 400
+
+    chunk_size = int(state["chunk_size"])
+    offset = idx * chunk_size
+
+    part_path = _upload_part_path(upload_id)
+    os.makedirs(part_path.parent, exist_ok=True)
+
+    # Write at offset (supports resume/out-of-order)
+    with open(part_path, "r+b" if part_path.exists() else "w+b") as f:
+        f.seek(offset)
+        f.write(chunk_file.read())
+
+    if idx not in state["received"]:
+        state["received"].append(idx)
+        state["received"].sort()
+        state["updated_at"] = datetime.utcnow().isoformat(timespec="seconds") + "Z"
+        sp.write_text(json.dumps(state), encoding="utf-8")
+
+    return jsonify({"ok": True, "upload_id": upload_id, "received_count": len(state["received"])})
+
+@app.post("/api/upload/complete/<upload_id>")
+def api_upload_complete(upload_id: str):
+    """Finalize a resumable upload: verify size, move into library/, index just that PDF."""
+    sp = _upload_state_path(upload_id)
+    part_path = _upload_part_path(upload_id)
+    if not sp.exists() or not part_path.exists():
+        return jsonify({"ok": False, "error": "not_found"}), 404
+
+    state = json.loads(sp.read_text(encoding="utf-8"))
+    total_size = int(state["total_size"])
+    relpath = _safe_relpath(state["relpath"])
+
+    # Verify size
+    if part_path.stat().st_size != total_size:
+        return jsonify({
+            "ok": False,
+            "error": "size_mismatch",
+            "expected": total_size,
+            "got": part_path.stat().st_size,
+        }), 400
+
+    dest = (LIB_DIR / relpath)
+    dest.parent.mkdir(parents=True, exist_ok=True)
+
+    # Atomic-ish move
+    tmp_dest = dest.with_suffix(dest.suffix + ".tmp")
+    shutil.move(str(part_path), str(tmp_dest))
+    tmp_dest.replace(dest)
+
+    # Cleanup state
+    try:
+        sp.unlink(missing_ok=True)
+    except Exception:
+        pass
+
+    # Index incrementally (fast)
+    idx_result = index_single_pdf(dest)
+
+    return jsonify({"ok": True, "saved_to": str(dest), "index": idx_result})
+
+
 @app.route("/assets/<path:filename>")
 def serve_asset(filename: str):
     # Note: this serves files from BASE_DIR, so putting soothing_hum.mp3